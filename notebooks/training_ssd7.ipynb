{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62266972-4177-4bcf-abb5-aee0a02469a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "### Append the project root directory to sys.path\n",
    "sys.path.append(os.path.abspath(\"/Users/prashal/dev/Face-Detector\"))\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, CSVLogger\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from models.keras_ssd7 import build_model\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.data_augmentation_chain_constant_input_size import DataAugmentationConstantInputSize\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "\n",
    "from misc_utils.utils import create_virtual_folder, format_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbd0af-b04f-4dad-98e0-8a566f94ec57",
   "metadata": {},
   "source": [
    "##### Define model configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ab8feb-7b38-40fd-af99-afd1173bccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 512\n",
    "img_width = 512\n",
    "img_channels = 3\n",
    "intensity_mean = 127.5  \n",
    "intensity_range = 127.5 \n",
    "n_classes = 1\n",
    "min_scale = 0.07\n",
    "max_scale = 0.9\n",
    "aspect_ratios = [0.5, 1.0, 2.0]\n",
    "two_boxes_for_ar1 = True\n",
    "steps = None\n",
    "offsets = None\n",
    "clip_boxes = False\n",
    "variances = [1.0, 1.0, 1.0, 1.0]\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b58c98-49c0-4b9e-b69a-47a6602025e6",
   "metadata": {},
   "source": [
    "##### Build the keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70f1aa8-d045-4eb1-bcae-f64f388ee3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssd = build_model(\n",
    "    image_size=(img_height, img_width, img_channels),\n",
    "    n_classes=n_classes,\n",
    "    mode=\"training\",\n",
    "    l2_regularization=0.0005,\n",
    "    min_scale=min_scale,\n",
    "    max_scale=max_scale,\n",
    "    aspect_ratios_global=aspect_ratios,\n",
    "    aspect_ratios_per_layer=None,\n",
    "    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "    steps=steps,\n",
    "    offsets=offsets,\n",
    "    clip_boxes=clip_boxes,\n",
    "    variances=variances,\n",
    "    normalize_coords=normalize_coords,\n",
    "    subtract_mean=intensity_mean,\n",
    "    divide_by_stddev=intensity_range,\n",
    ")\n",
    "\n",
    "adam = Adam(learning_rate=0.001)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "ssd.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdb7039-e2a3-43aa-a8fc-af9014981a34",
   "metadata": {},
   "source": [
    "##### Set up the data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfad2bb5-0de8-437c-b15c-8ebf03602ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formatted file succesfully saved to /Users/prashal/dev/Face-Detector/dataset/train/train_bbox.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create symlinks to train-set\n",
    "src_folder_train = os.path.abspath(\"../wider_face/data/WIDER_train/images\")\n",
    "virtual_folder_train = os.path.abspath(\"../dataset/train/images\")\n",
    "create_virtual_folder(src_folder_train, virtual_folder_train)\n",
    "### Format training set ground truth labels\n",
    "train_labels_file = os.path.abspath(\"../wider_face/data/wider_face_split/wider_face_train_bbx_gt.txt\")\n",
    "formatted_train_labels_file = os.path.abspath(\"../dataset/train/train_bbox.csv\")\n",
    "format_labels(train_labels_file, formatted_train_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9b873f-bd54-465f-8336-a24411c12021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Formatted file succesfully saved to /Users/prashal/dev/Face-Detector/dataset/val/val_bbox.csv'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create symlinks to validation-set\n",
    "src_folder_val = os.path.abspath(\"../wider_face/data/WIDER_val/images\")\n",
    "virtual_folder_val = os.path.abspath(\"../dataset/val/images\")\n",
    "create_virtual_folder(src_folder_val, virtual_folder_val)\n",
    "### Format validation set ground truth labels\n",
    "val_labels_file = os.path.abspath(\"../wider_face/data/wider_face_split/wider_face_val_bbx_gt.txt\")\n",
    "formatted_val_labels_file = os.path.abspath(\"../dataset/val/val_bbox.csv\")\n",
    "format_labels(val_labels_file, formatted_val_labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "282d2e6c-1079-4889-9101-795426cfef0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the training dataset: 12876\n",
      "Number of images in the validation dataset: 3226\n"
     ]
    }
   ],
   "source": [
    "### Instantiate data generators\n",
    "train_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=False, hdf5_dataset_path=None)\n",
    "\n",
    "### Images\n",
    "images_train = virtual_folder_train\n",
    "images_val = virtual_folder_val\n",
    "\n",
    "### Ground truth\n",
    "labels_train = formatted_train_labels_file\n",
    "labels_val = formatted_val_labels_file\n",
    "\n",
    "train_dataset.parse_csv(\n",
    "    images_dir=images_train,\n",
    "    labels_filename=labels_train,\n",
    "    input_format=[\"image_name\", \"xmin\", \"xmax\", \"ymin\", \"ymax\", \"class_id\"],\n",
    "    ### There are 4 images with only background in the train set. Their class id is set to 0. Therefore only include class id 1 (face).\n",
    "    include_classes=[1],\n",
    ")\n",
    "\n",
    "val_dataset.parse_csv(\n",
    "    images_dir=images_val,\n",
    "    labels_filename=labels_val,\n",
    "    input_format=[\"image_name\", \"xmin\", \"xmax\", \"ymin\", \"ymax\", \"class_id\"],\n",
    "    include_classes=[1],\n",
    ")\n",
    "\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size = val_dataset.get_dataset_size()\n",
    "\n",
    "print(f\"Number of images in the training dataset: {train_dataset_size}\")\n",
    "print(f\"Number of images in the validation dataset: {val_dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b15744-e6bd-4570-b766-a26906b9fa28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# data_augmentation_chain = DataAugmentationConstantInputSize(random_brightness=(-48, 48, 0.5),\n",
    "#                                                             random_contrast=(0.5, 1.8, 0.5),\n",
    "#                                                             random_saturation=(0.5, 1.8, 0.5),\n",
    "#                                                             random_hue=(18, 0.5),\n",
    "#                                                             random_flip=0.5,\n",
    "#                                                             random_translate=((0.03,0.5), (0.03,0.5), 0.5),\n",
    "#                                                             random_scale=(0.5, 2.0, 0.5),\n",
    "#                                                             n_trials_max=3,\n",
    "#                                                             clip_boxes=True,\n",
    "#                                                             overlap_criterion='area',\n",
    "#                                                             bounds_box_filter=(0.3, 1.0),\n",
    "#                                                             bounds_validator=(0.5, 1.0),\n",
    "#                                                             n_boxes_min=1,\n",
    "#                                                             background=(0,0,0))\n",
    "\n",
    "resize = Resize(height=img_height, width=img_width)\n",
    "\n",
    "predictor_sizes = [ssd.get_layer('classes4').output.shape[1:3],\n",
    "                   ssd.get_layer('classes5').output.shape[1:3],\n",
    "                   ssd.get_layer('classes6').output.shape[1:3],\n",
    "                   ssd.get_layer('classes7').output.shape[1:3]]\n",
    "\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    min_scale=min_scale,\n",
    "                                    max_scale=max_scale,\n",
    "                                    aspect_ratios_global=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.3,\n",
    "                                    normalize_coords=normalize_coords)\n",
    "\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[resize],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images', 'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images', 'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88808963-8fba-43da-8c61-402fce19dd30",
   "metadata": {},
   "source": [
    "##### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e14f6bc2-6d7e-4401-b610-9dd07cdfaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create directories to save model checkpoints and csv loggers\n",
    "os.makedirs(name='../tmp/checkpoints', exist_ok=True)\n",
    "os.makedirs(name='../tmp/runs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79c0188-0e7a-478c-9125-9c6b8a8fc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define model callbacks\n",
    "model_checkpoint = ModelCheckpoint(filepath='../tmp/checkpoints/ssd7_epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.keras',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   save_freq='epoch')\n",
    "\n",
    "csv_logger = CSVLogger(filename='../tmp/runs/ssd_7_training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=0.0,\n",
    "                               patience=10,\n",
    "                               verbose=1)\n",
    "\n",
    "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                         factor=0.2,\n",
    "                                         patience=8,\n",
    "                                         verbose=1,\n",
    "                                         min_delta=0.001,\n",
    "                                         cooldown=0,\n",
    "                                         min_lr=0.00001)\n",
    "\n",
    "callbacks = [model_checkpoint, \n",
    "             csv_logger, \n",
    "             early_stopping, \n",
    "             reduce_learning_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef4afd-3e4f-4fa0-aa8c-640d6ac265c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "final_epoch = 20\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history = ssd.fit(train_generator,\n",
    "                  steps_per_epoch=steps_per_epoch,\n",
    "                  epochs=final_epoch,\n",
    "                  callbacks=callbacks,\n",
    "                  validation_data=val_generator,\n",
    "                  validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                  initial_epoch=initial_epoch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02f3b96-6e12-4e1a-95e0-0591f4adbf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot training curves\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.plot(history.history['loss'], label=['loss'])\n",
    "plt.plot(history.history['val_loss'], label=['validation loss'])\n",
    "plt.legend(loc='upper right', prop={'size':24})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5cebc-afdb-4c5a-9ca4-f0270d3b5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the history object in JSON format\n",
    "\n",
    "history_dict = history.history\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "with open(f'../tmp/runs/history {timestamp}.json', 'w') as f:\n",
    "    json.dump(history_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac04bd9-2d7e-4164-97d2-2cd4ff9385bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
